apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ .Values.inferenceService.name }}
  annotations:
    openshift.io/display-name: {{ .Values.inferenceService.name }}
    serving.kserve.io/deploymentMode: RawDeployment
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: {{ .Values.inferenceService.predictor.maxReplicas }}
    minReplicas: {{ .Values.inferenceService.predictor.minReplicas }}
    model:
      modelFormat:
        name: vLLM
      name: {{ .Values.inferenceService.predictor.model.name }}
      resources:
        limits:
          {{- toYaml .Values.inferenceService.predictor.model.resources.limits | nindent 10 }}
        requests:
          {{- toYaml .Values.inferenceService.predictor.model.resources.requests | nindent 10 }}
      runtime: {{ .Values.inferenceService.predictor.model.runtime }}
      args:
        {{- toYaml .Values.inferenceService.predictor.model.args | nindent 8 }}
      storageUri: {{ .Values.inferenceService.predictor.model.storage.uri }}
    {{- with .Values.inferenceService.predictor.tolerations }}
    tolerations:
      {{- toYaml . | nindent 6 }}
    {{- end }}
